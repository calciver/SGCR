{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SGCR Colab Version 4.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/calciver/SGCR/blob/master/SGCR_Colab_Version_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdGelX5HvnRL",
        "colab_type": "text"
      },
      "source": [
        "# Download the dataset and unzip"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n37aycLV6ZiA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -q patool\n",
        "import patoolib\n",
        "import os\n",
        "\n",
        "#Organise our files\n",
        "ROOT_DIR = os.path.abspath(\"/content\")\n",
        "!git clone https://github.com/calciver/SGCR.git  \n",
        "WORK_DIR = os.path.abspath(\"/content/SGCR\")\n",
        "os.chdir(WORK_DIR)\n",
        "\n",
        "\n",
        "patoolib.extract_archive(\"train_and_val_images.zip\")  \n",
        "IMAGE_DIR = os.path.abspath(\"/content/radiology/train_and_val_images\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SohN7_En3tEW",
        "colab_type": "text"
      },
      "source": [
        "# Load the libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rtJgah33vi8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras import optimizers\n",
        "import csv\n",
        "from keras.layers import Dropout, Activation, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from PIL import Image\n",
        "from keras.utils import to_categorical\n",
        "from keras.callbacks import TensorBoard\n",
        "import time\n",
        "from numpy import random\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "ROOT_DIR = os.path.abspath(\"/content\")\n",
        "!git clone https://github.com/calciver/SGCR.git  \n",
        "WORK_DIR = os.path.abspath(\"/content/SGCR\")\n",
        "os.chdir(WORK_DIR)\n",
        "\n",
        "\n",
        "csv_files_dir = os.path.abspath('/content/SGCR')\n",
        "train_image_file_dir = os.path.abspath('/content/SGCR/train_and_val_images/train')\n",
        "val_image_file_dir = os.path.abspath('/content/SGCR/train_and_val_images/val')\n",
        "\n",
        "os.mkdir('models')\n",
        "model_directory = os.path.abspath(\"/content/SGCR/models\")\n",
        "\n",
        "val_csv_filename = 'val.csv'\n",
        "train_csv_filename = 'train.csv'\n",
        "\n",
        "#Number of epochs\n",
        "num_epochs = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWVYCYrh_mmn",
        "colab_type": "text"
      },
      "source": [
        "# Process and Load the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSpBQcbt_mGt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Gets csv files directory and csv filename and returns rows in csv files which are image names and lables\n",
        "def load_csv (csv_files_dir, csv_filename) :\n",
        "    csv_rows = []\n",
        "    with open(csv_files_dir + '/' + csv_filename, newline='') as csvFile:\n",
        "        reader = csv.reader(csvFile, delimiter=',')\n",
        "        for row in reader:\n",
        "            csv_rows.append(row)\n",
        "    #print('csv_rows[-1] : ', csv_rows[-1], len(csv_rows))\n",
        "    return csv_rows\n",
        "\n",
        "#returns numpy arrays of images and labels for each batch\n",
        "def load_data (batch, image_dir):\n",
        "    image_array = []\n",
        "    label_array = []\n",
        "    for b in batch:\n",
        "        image_array.append(np.asarray(Image.open(image_dir + '/' + b[0])))\n",
        "        label_array.append(b[1])\n",
        "    labels = np.array(label_array)\n",
        "    images = np.array(image_array)\n",
        "    return images, labels\n",
        "\n",
        "#Preparing training data\n",
        "\n",
        "train_imagename_label_pairs = load_csv(csv_files_dir, train_csv_filename)\n",
        "#shuffle images and labels foe each epoch\n",
        "random.shuffle(train_imagename_label_pairs)\n",
        "[train_images, train_labels ] = load_data(train_imagename_label_pairs, train_image_file_dir)\n",
        "\n",
        "#number of training data size\n",
        "batch_size_train= len(train_imagename_label_pairs)\n",
        "image_shape = train_images.shape\n",
        "train_images2 = np.reshape(train_images, [-1, image_shape[1], image_shape[2], 1])\n",
        "train_labels1 = to_categorical(train_labels)\n",
        "train_labels2 = np.reshape(train_labels1, [batch_size_train, 2])\n",
        "\n",
        "#Preparing validation data\n",
        "\n",
        "val_imagename_label_pairs = load_csv(csv_files_dir, val_csv_filename)\n",
        "random.shuffle(val_imagename_label_pairs)\n",
        "[val_images, val_labels] = load_data(val_imagename_label_pairs, val_image_file_dir)\n",
        "\n",
        "#number of training validation size\n",
        "batch_size_val = len(val_imagename_label_pairs)\n",
        "image_shape = val_images.shape\n",
        "val_images2 = np.reshape(val_images, [-1, image_shape[1], image_shape[2], 1])\n",
        "val_labels1 = to_categorical(val_labels)\n",
        "val_labels2 = np.reshape(val_labels1, [batch_size_val, 2])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1zl_cmq1Ebo",
        "colab_type": "text"
      },
      "source": [
        "# Build the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynqwDXvm1Fck",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#This class is used to merge traing accuracy and validation accuracy graphs in Tensorboard\n",
        "\n",
        "class TrainValTensorBoard(TensorBoard):\n",
        "    def __init__(self, log_dir='/logs', **kwargs):\n",
        "        # Make the original `TensorBoard` log to a subdirectory 'training'\n",
        "        training_log_dir = os.path.join(log_dir, 'training')\n",
        "        super(TrainValTensorBoard, self).__init__(training_log_dir, **kwargs)\n",
        "\n",
        "        # Log the validation metrics to a separate subdirectory\n",
        "        self.val_log_dir = os.path.join(log_dir, 'validation')\n",
        "\n",
        "    def set_model(self, model):\n",
        "        # Setup writer for validation metrics\n",
        "        self.val_writer = tf.summary.FileWriter(self.val_log_dir)\n",
        "        super(TrainValTensorBoard, self).set_model(model)\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        # Pop the validation logs and handle them separately with\n",
        "        # `self.val_writer`. Also rename the keys so that they can\n",
        "        # be plotted on the same figure with the training metrics\n",
        "        logs = logs or {}\n",
        "        val_logs = {k.replace('val_', ''): v for k, v in logs.items() if k.startswith('val_')}\n",
        "        for name, value in val_logs.items():\n",
        "            summary = tf.Summary()\n",
        "            summary_value = summary.value.add()\n",
        "            if isinstance(value, np.ndarray):\n",
        "                summary_value.simple_value = value.item()\n",
        "            else:\n",
        "                summary_value.simple_value = value\n",
        "            summary_value.tag = name\n",
        "            self.val_writer.add_summary(summary, epoch)\n",
        "        self.val_writer.flush()\n",
        "\n",
        "        # Pass the remaining logs to `TensorBoard.on_epoch_end`\n",
        "        logs = {k: v for k, v in logs.items() if not k.startswith('val_')}\n",
        "        super(TrainValTensorBoard, self).on_epoch_end(epoch, logs)\n",
        "\n",
        "    def on_train_end(self, logs=None):\n",
        "        super(TrainValTensorBoard, self).on_train_end(logs)\n",
        "        self.val_writer.close()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#ploting 4 different categories\n",
        "def show_samples(input_image , input_pairs  ,image_dir):\n",
        "\n",
        "    correct_pneumonia=[]\n",
        "    incorrect_pneumonia=[]\n",
        "    correct_normal=[]\n",
        "    incorrect_normal=[]\n",
        "    prediction = np.empty(0)\n",
        "#model.predict returns 2 values which are predictions of each label\n",
        "    pred =model.predict(x=input_image, batch_size=None , verbose=0, steps=None)\n",
        "    prediction = np.append(prediction,pred)\n",
        "    prediction_list = prediction.tolist()\n",
        "    zero_labels_predicted = prediction_list[::2]\n",
        "    one_labels_predicted = prediction_list[1::2]\n",
        "\n",
        "    for i in range(len(input_pairs)) :\n",
        "\n",
        "        if input_pairs[i][1] == '1' :\n",
        "            if one_labels_predicted[i] > 0.50 :\n",
        "                print(\"correct_pneumonia : \" , input_pairs[i][0])\n",
        "                correct_pneumonia.append(input_pairs[i][0])\n",
        "            elif one_labels_predicted[i] <0.50 :\n",
        "                incorrect_pneumonia.append(input_pairs[i][0])\n",
        "\n",
        "        elif input_pairs[i][1] == '0' :\n",
        "            if zero_labels_predicted[i] > 0.50 :\n",
        "                correct_normal.append(input_pairs[i][0])\n",
        "                print(\"correct normal : \", input_pairs[i][0])\n",
        "            elif zero_labels_predicted[i] < 0.50 :\n",
        "                incorrect_normal.append(input_pairs[i][0])\n",
        "\n",
        "#ploting images\n",
        "    if correct_pneumonia != [] :\n",
        "        plt.subplot(2, 2, 1)\n",
        "        img = plt.imread(image_dir +'/' + correct_pneumonia[0])\n",
        "        plt.imshow(img, cmap='gray')\n",
        "        plt.title('correct_pneumonia')\n",
        "\n",
        "    if incorrect_pneumonia != [] :\n",
        "        plt.subplot(2, 2, 2)\n",
        "        img = Image.open(image_dir + '/' + incorrect_pneumonia[0])\n",
        "        plt.imshow(img, cmap='gray')\n",
        "        plt.title('incorrect_pneumonia')\n",
        "\n",
        "    if correct_normal !=[] :\n",
        "        plt.subplot(2, 2, 3)\n",
        "        img = Image.open(image_dir + '/' + correct_normal[0])\n",
        "        plt.imshow(img, cmap='gray')\n",
        "        plt.title('correct_normal')\n",
        "\n",
        "    if incorrect_normal != [] :\n",
        "        plt.subplot(2, 2, 4)\n",
        "        img = Image.open(image_dir + '/' + incorrect_normal[0])\n",
        "        plt.imshow(img, cmap='gray')\n",
        "        plt.title('incorrect_normal')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "#saving model\n",
        "def save_model(model):\n",
        "    # serialize model to JSON\n",
        "    model_json =  model.to_json()\n",
        "\n",
        "    if not os.path.exists(model_directory):\n",
        "        os.mkdir(model_directory)\n",
        "\n",
        "    model_filename_json =  model_directory+Name+ \".json\"\n",
        "    print(model_filename_json)\n",
        "\n",
        "    model_filename_h5 = model_directory +Name+ \".h5\"\n",
        "\n",
        "    with open(model_filename_json, \"w\") as json_file:\n",
        "        json_file.write(model_json)\n",
        "    # serialize weights to HDF5\n",
        "    model.save_weights(model_filename_h5)\n",
        "    print(\"Saved model to disk\")\n",
        "\n",
        "\n",
        "\n",
        "#Model Architecture\n",
        "\n",
        "Name = 'updated_model-{}'.format(int(time.time()))\n",
        "tensorboard = TrainValTensorBoard(log_dir='logs/{}'.format(Name))\n",
        "\n",
        "#set up a sequential model\n",
        "model = Sequential()\n",
        "#first convolutional layer with 8 channels and kenel size of 7*7\n",
        "model.add(Conv2D(8, (7, 7), input_shape=(512, 512, 1), kernel_initializer=\"glorot_normal\"))\n",
        "# tanh activation function\n",
        "model.add(Activation('tanh'))\n",
        "#polling layer using Maxpooling of window size 2*2\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(16, (4, 4), kernel_initializer=\"glorot_normal\"))\n",
        "model.add(Activation('tanh'))\n",
        "model.add(MaxPooling2D(pool_size=(3, 3)))\n",
        "\n",
        "model.add(Conv2D(32, (3, 3), kernel_initializer=\"glorot_normal\"))\n",
        "model.add(Activation('tanh'))\n",
        "model.add(MaxPooling2D(pool_size=(3, 3)))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(64, kernel_initializer=\"glorot_normal\"))\n",
        "model.add(Activation('tanh'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(32, kernel_initializer=\"glorot_normal\"))\n",
        "model.add(Activation('tanh'))\n",
        "\n",
        "model.add(Dense(2))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "#Summarize Model and Visualize Model\n",
        "model.summary()\n",
        "\n",
        "#Configures the model for training using  existing loss function 'categorical_crossnetropy' .\n",
        "#'Adam' optimizer with learning rate of 0.0005.\n",
        "#and 'categorical_accuracy' as metrices to be evaluated by the model during training and testing.\n",
        "model.compile(loss=\"categorical_crossentropy\",\n",
        "              optimizer=optimizers.Adam(lr=0.0005, beta_1=0.9, beta_2=0.999, decay=0.00001, amsgrad=True),\n",
        "              metrics=[\"categorical_accuracy\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thz-khi1_ZKh",
        "colab_type": "text"
      },
      "source": [
        "# Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OO85k7HB_aiU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = model.fit(x=train_images2, y=train_labels2,  epochs = num_epochs ,batch_size=10,\n",
        "                    validation_data=[val_images2, val_labels2],\n",
        "                     verbose=2 , shuffle=True, callbacks=[tensorboard])\n",
        "\n",
        "\n",
        "#ploting 4 samples of each catagory : correct_pneumonia ,incorrect_pneumonia, correct_normal, incorrect_normal\n",
        "#if you can not see some of these categories it means model did not predict any samples of that categories\n",
        "show_samples(train_images2 ,train_imagename_label_pairs , image_dir = train_image_file_dir)\n",
        "\n",
        "\n",
        "#Saving Model\n",
        "dir = os.getcwd()\n",
        "save_model(model)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}